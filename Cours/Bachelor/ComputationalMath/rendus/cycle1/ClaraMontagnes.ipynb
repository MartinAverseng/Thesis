{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='./figures/logo-ecole-polytechnique-ve.jpg' style='position:absolute; top:0; right:0;' width='100px' height='' alt='' />\n",
    "\n",
    "<center>**Bachelor of Ecole Polytechnique**</center>\n",
    "<center>Computational Mathematics, year 1, semester 2</center>\n",
    "<center>Author: Aline Lefebvre-Lepot</center>\n",
    "\n",
    "# Introduction to Computational Mathematics\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"./figures/ApproxPi.png\" alt=\"Pi\" style=\"width: 570px;\"/>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div markdown=1 class=Abstract>\n",
    "In this chapter, we introduce the fundamental notion of **approximation**, which is the basis of Computational Mathematics. Approximation induces errors that have to be controlled. We focus on two kinds of errors: **round-off errors** due to machine representation of numbers and **truncation errors** due to mathematical approximation. Through examples, we show that mathematical analysis allows to study the behavior of errors and we introduce the notions of **convergence** and **speed of convergence**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Machine representation of numbers: round-off error](#RoundOff)\n",
    "- [Mathematical approximations: truncation error](#trunc)\n",
    "- [Total numerical error](#total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loading python libraries\n",
    "\n",
    "# necessary to display plots inline:\n",
    "%matplotlib inline   \n",
    "\n",
    "# load the libraries\n",
    "import matplotlib.pyplot as plt # 2D plotting library\n",
    "import numpy as np              # package for scientific computing  \n",
    "\n",
    "from math import *              # package for mathematics (pi, arctan, sqrt, factorial ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers are essential to simulate and model complex phenomenons in several domains as physics, biology, economy... Several steps have to be fulfilled to go from the original phenomenon to the simulation result:\n",
    "- Understand the original phenomenon: physics, economy, biology...\n",
    "- Write a mathematical model: equations, differential equations, integrals, probabilities...\n",
    "- Analyse the mathematical model: is there a solution ? how does it behave ? what are its properties ? Are there analytical solutions ?\n",
    "- Design a numerical method: find a method and an algorithm to approximate the solutions to the mathematical model.\n",
    "- Analyse the numerical method: does it give precise approximations ? how precise are they ?\n",
    "- Implement the numerical method on computer\n",
    "- Visualise and analyse the numerical results\n",
    "\n",
    "Most of these steps implies approximations: \n",
    "\n",
    "- the original phenomenon is approximated by the mathematical model\n",
    "- the mathematical model is approximated by the numerical method\n",
    "- the numerical method's result is approximated by the computer\n",
    "\n",
    "To be confident in the results obtained by computer simulations, one has to be able to say how precise is the approximation at each of these steps: it is essential to evaluate the corresponding error (for example, providing margin of error). Note that, even if one wants the results to be precise, it is also necessary to design efficient numerical methods, to compute these results in a reasonable time.\n",
    "\n",
    "**Computational mathematics** is the domain of mathematics which aims to design precise and efficient numerical methods, in order to approximate solutions to a given mathematical problem. For the same problem, various numerical methods and algorithms can be proposed. A great part of the research in computational mathematics is devoted to estimate the error behavior induced by the numerical models, in order to be able to compare the different algorithms. \n",
    "\n",
    "Before going further, we recall the definition of the absolute and relative error which are used to evaluate the precision of a given approximation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the following definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Def\">\n",
    "If $x^*$ is an approximation to $x$,   \n",
    "\n",
    "- the **absolute error** between $x$ and $x^*$ is ... \n",
    "- the **relative error** between $x$ and $x^*$ is ... \n",
    "\n",
    "where $|\\,x\\,|$ is the absolute value of $x$.\n",
    "\n",
    "The relative error can be expressed in term of percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div markdown=1 class=\"Ex\">\n",
    "Approximation of $\\pi$:\n",
    "\n",
    "| Approximation               |    Absolute error   |Relative error| Relative error in $\\%$| \n",
    "|:-----------:|:-----------------:|:---------:|\n",
    "| $x^*=22/7$  |  $1.26\\times10^{-3}$ |  $4.02\\times 10^{-4}$| $4.02\\times 10^{-2}\\%$ |\n",
    "| $x^*=3.1416$  |  $7.35\\times10^{-6}$ |  $2.34\\times 10^{-6}$ | $2.34\\times 10^{-4}\\%$ |\n",
    "\n",
    "\n",
    "- If $x^*$ approximates $\\pi$ with **absolute error** at most $10^{-3}$, it must belong to the interval $$[\\pi-10^{-3},\\pi+10^{-3}] \\sim [3.1406, 3.1426].$$\n",
    "- If $x^*$ approximates $\\pi$ with **relative error** at most $10^{-3}$, it must belong to the interval $$[\\pi(1-10^{-3}),\\pi(1+10^{-3})] \\sim [3.1385, 3.1447].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div markdown=1 class=\"Rmk\">\n",
    "The same relative error can occur for widely varying absolute errors :\n",
    "\n",
    "| Value to be estimated| &nbsp;&nbsp; Approximation &nbsp;&nbsp;| Absolute error|Relative error|\n",
    "|:--------------------:|:-----------------------:|:------------------:|:----------------:|\n",
    "| $x=1\\times 10^1$     | $x^*=1.1\\times 10^1$    |$1\\times 10^{-1}$   |$1\\times 10^{-1}$ |\n",
    "| $x=1\\times 10^{-3} $ | $x^*=1.1\\times 10^{-3}$ | $1.1\\times 10^{-4}$|$1\\times 10^{-1}$ |\n",
    "| $x=1\\times 10^4 $    | $x^*=1.1\\times 10^4$    | $1.1\\times 10^{3}$ |$1\\times 10^{-1}$ |\n",
    "\n",
    "The relative error is more meaningful as a measure of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"RoundOff\"></a>\n",
    "## Machine representation of numbers: round-off error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Zuse_z3.jpg\" alt=\"Zuse\" style=\"width: 300px;\"/>\n",
    "  \n",
    ">**Konrad Zuse (1910-1995) and the Z3-computer.**\n",
    ">The Z3-computer is said to be the first programmable computer. It was built by Konrad Zuse in 1941. It is based on binary floating-point numbers, is programmable with loops, has a memory and a computation unit. It is often refered as the ancestor of today's computers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arithmetic performed by a computer is different from the arithmetic you use in your theoretical courses. The reason for that is that in the computational world, **the represented numbers can only have a fixed, finite number of digits**.\n",
    "\n",
    "As a consequence, numbers as $\\pi$ or $\\sqrt{2}$ cannot be represented exactly and are approximated. Then, for example, $(\\sqrt{2})^2$ will not be exactly equal to $2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the following script to print $(\\sqrt 2)^2$. Try other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqrt(2)^2 = 2.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "## sqrt(2)^2 is not equal to 2\n",
    "\n",
    "a = sqrt(2)**2\n",
    "print('sqrt(2)^2 =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, the results of the computer calculations are sufficiently precise for the purpose the user has in mind. \n",
    "\n",
    "However, that is not always true. The user has to keep in mind that real numbers are approximated and to be aware of the possible problems that it can produce:\n",
    "- Computers have limited magnitude and precision to represent numbers\n",
    "- Some arithmetical manipulations are highly sensitive to this approximated representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine representation of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before presenting the number representation in computers, let us first recall what is a *number system*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\"> The system we are most familiar with is the *decimal* system (or the *base-10* system). In this system, the numbers are decomposed in sums of powers of $10$:\n",
    "\n",
    "$$\n",
    "[6743.7]_{10} = 6\\times 10^3 + 7\\times 10^2 +4\\times 10^1 + 3\\times 10^0 + 7\\times 10^{-1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\"> Computers use the *binary* system (or *base-2* system) to represent numbers:\n",
    "\n",
    "$$\n",
    "[1011.1]_{2} = 1\\times 2^3 + 0\\times 2^2 +1\\times 2^1 + 1\\times 2^0 + 1\\times 2^{-1} \\quad(= [11.5]_{10})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generaly, one can define a system number for any base $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Def\">\n",
    "**Base-$\\beta$ number system. Positional representation.**\n",
    "\n",
    "For any integer $\\beta\\geq 2$, the *positional representation* $x_\\beta$ of a real $x$ with a finite number of digits is\n",
    "\n",
    "$$\n",
    "x_\\beta = (-1)^s [x_n\\,x_{n-1}\\ldots x_1\\,x_0\\, . \\, x_{-1}x_{-2}\\ldots x_{-m}]_\\beta,\n",
    "$$\n",
    "with\n",
    "$$ s\\in\\{0,1 \\},\\quad 0\\leq x_k<\\beta \\quad\\text{ for }\\quad k=-m\\ldots n, \\quad\\text{ and }\\quad x_n\\neq 0.\n",
    "$$\n",
    "\n",
    "This representation stands for the number\n",
    "$$\n",
    "x = (-1)^s\\sum_{k=-m}^n x_k \\beta^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fractional number can have a finite number of digits in a base and an infinite number of digits in another base. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\"> The fraction $x=1/3$ has infinite digits in base $10$ with $x_{10}=[0.333\\ldots]_{10}$ while it has only one digit in base $3$ : $x_3=[0.1]_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent numbers in computers, an exponent representation of reals is used. For example, we can write in base $10$\n",
    "\n",
    "$$\n",
    "x=-47.258 = (-1)^1 \\times 0.47258 \\times 10^2.\n",
    "$$\n",
    "\n",
    "It can be represented by storing the 4 following values\n",
    "- the **exponent** : $b=2$ \n",
    "- the ** sign of the exponent** :  $s_b=0$ \n",
    "- the **mantissa** :  $a=47258$\n",
    "- the **sign** : $s_a=1$\n",
    "\n",
    "So that \n",
    "$$\n",
    "x=(-1)^{s_a} \\times [0.a]_{10} \\times 10^{(-1)^{s_b}[b]_{10}}.\n",
    "$$\n",
    "\n",
    "Note that we could also have written\n",
    "\n",
    "$$\n",
    "- 47.258 = (-1)^1 \\times 0.0047258 \\times 10^4\n",
    "$$\n",
    "\n",
    "which would have given $s_b=0$, $b=4$, $s_a=1$ and $a=0047258$. \n",
    "\n",
    "To ensure uniqueness, the representation is normalized imposing that the first digit of $a$ is different from $0$. We say that all the digits of $a$ are **significant**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Def\">\n",
    "**Normalized exponent representation.**\n",
    "\n",
    "Let $\\beta>0$ be a given basis and $x$ a real. Then, $x$ can be written in a unique way as\n",
    "\n",
    "$$\n",
    "x=(-1)^{s_a} \\times [0.a]_{\\beta} \\times \\beta^{(-1)^{s_b}[b]_\\beta}\n",
    "$$\n",
    "\n",
    "where the first digit of $a$ is different from $0$. This is called the *normalized exponent representation* for $x$ in base $\\beta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One wants to use this normalized exponent representation to represent reals in computers by storing the list $(s_b,b,s_a,a)$. However, computer have a fixed number of places $N$, the *word length*, to store a number. Two places are used to store the signs $s_b$, $s_a$. It remains $N-2$ places for the exponenent $b$ and the mantissa $a$. For a given machine, the number of places available for the exponent and the mantissa are given. If we call them respectively $n$ and $m$, we have $N=m+n+2$. Due to this length restriction, only a finite set of reals can be represented: the so-called **machine numbers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Def\">\n",
    "**Machine numbers.**\n",
    "\n",
    "Let $n$ and $m$ be given integers and $\\beta>0$ be a biven basis ($n$, $m$ and $\\beta$ are given by the machine). Suppose that a real $x$ has the following normalized exponent representation in base $\\beta$\n",
    "\n",
    "$$\n",
    "x=(-1)^{s_a} \\times [0.a]_{\\beta} \\times \\beta^{(-1)^{s_b}[b]_\\beta}\n",
    "$$\n",
    "\n",
    "with the length of $b$ and $a$ respectively lower than $n$ and $m$.\n",
    "\n",
    "Then, $x$ is called a *machine number* for the couple $(n,m)$ and the base $\\beta$. It can be exaclty represented by the word of size $N=n+m+2$\n",
    " \n",
    "$$|\\quad s_b \\quad|\\quad b \\quad|\\quad s_a\\quad|\\quad a \\quad|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\"> For $\\beta=10$, $n=2$, $m=5$\n",
    "- The number $x=-47.258=-0.47258\\times 10^2$ is represented by the word of length $N=9$ $$|\\quad 0 \\quad|\\quad 02 \\quad|\\quad 1 \\quad|\\quad 47258\\quad|$$\n",
    "- The number $x=0.0836=0.836\\times 10^{-1}$ is represented by the word of length $N=9$ $$|\\quad 1 \\quad|\\quad 01 \\quad|\\quad 0 \\quad|\\quad 83600\\quad|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already said, the set $A$ of numbers which are representable in a given machine is finite and most of the real numbers cannot be exaclty represented. These real numbers have to be approximated by a machine number contained in $A$. It is natural to suppose that a real $x\\notin A$ should be approximated by $rd(x)\\in A$ where $rd(x)$ is the best approximation of $x$ by an element of $A$. Such an approximation can be achieved by **rounding** and is called the floating-point representation of $x$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Def\">\n",
    "**Floating-point representation and round-off error.**\n",
    "\n",
    "Let $n$, $m$ and $\\beta$ be given and denote by $A$ the corresponding set of machine numbers that can be exactly represented. Consider a real $x\\notin A$. Its *floating-point representation* $rd(x)$ is the best approximation of $x$ by an element of $A$:\n",
    "\n",
    "$$\n",
    "rd(x)\\in A, \\quad \\text{ and } \\quad \\forall y\\in A, \\,\\,|x-rd(x)|\\leq|x-y|.\n",
    "$$\n",
    "\n",
    "The error produced replacing a number $x$ by its floating-point representation $rd(x)$ is called the **round-off error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\"> For $\\beta=10$, $n=2$, $m=5$, the number $\\pi=3.14159265\\ldots$ can be written in the decimal normalized representation as \n",
    "\n",
    "$$\\pi=0.314159265\\ldots\\times 10^1.$$\n",
    "\n",
    "For $m=5$, the decimal machine number approximating $\\pi$ is\n",
    "\n",
    "$$rd(\\pi)=0.31416\\times 10^1=3.1416,$$\n",
    "\n",
    "and $\\pi$ is represented by the word of length $N=9$ \n",
    "\n",
    "$$|\\quad 0 \\quad|\\quad 01 \\quad|\\quad 1 \\quad|\\quad 31416\\quad|.$$\n",
    "\n",
    "The corresponding relative round-off error is\n",
    "\n",
    "$$\n",
    "\\left|\\frac{\\pi-rd(\\pi)}{\\pi}\\right| \\approx 2.34\\times 10^{-6}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are working with decimal representations of numbers and that the number of digits $m$ for the mantissa is given. If the normalized exponent representation of $x$ is\n",
    "\n",
    "$$x=0.a_1a_2...a_m a_{m+1}... \\times 10^n,$$\n",
    "\n",
    "then, its floating-point representation is\n",
    "\n",
    "$$rd(x)=0.a_1a_2...\\tilde a_m \\times 10^n,$$\n",
    "\n",
    "where $\\tilde a_m=a_m$ if $a_{m+1}<5$ and $\\tilde a_m=a_m+1$ if $a_{m+1}\\geq 5$. The corresponding relative round-off error is\n",
    "\n",
    "\\begin{align}\n",
    "\\left|\\frac{x-rd(x)}{x}\\right|&=\\left|\\frac{(0.a_1a_2...a_m a_{m+1}...-0.a_1a_2...\\tilde a_m)\\times 10^n}{0.a_1a_2...a_m a_{m+1}...\\times 10^n}\\right|\\\\\n",
    "&=\\left|\\frac{(a_m . a_{m+1}...-\\tilde a_m)\\times 10^{n-m}}{0.a_1a_2...a_m a_{m+1}...\\times 10^n}\\right|=\\left|\\frac{a_m . a_{m+1}...-\\tilde a_m}{0.a_1a_2...a_m a_{m+1}...}\\right|\\times 10^{-m}.\n",
    "\\end{align}\n",
    "\n",
    "Since $a_1\\neq 0$, the denominator is greater than $0.1$ and the rounding procedure ensures that the numerator is lower than $0.5$, which gives\n",
    "\n",
    "$$\n",
    "\\left|\\frac{x-rd(x)}{x}\\right| \\leq \\frac{0.5}{0.1}\\times 10^{-m} = 5 \\times 10^{-m}.\n",
    "$$\n",
    "\n",
    "If $\\varepsilon=5 \\times 10^{-m}$, then we have the following result:\n",
    "\n",
    "$$\n",
    "\\forall x\\in \\mathbb{R},\\quad rd(x) \\,=\\, x \\, (1-\\alpha) \\quad \\text{ with } |\\alpha|<\\varepsilon\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Def\">\n",
    "**Machine precision.**\n",
    "\n",
    "Consider the floating-point representation for the couple $(n,m)$ and base $\\beta$. The *machine precision* $\\varepsilon$ is the smallest positive real such that\n",
    "\n",
    "$$\n",
    "\\forall x\\in \\mathbb{R},\\quad rd(x) \\,=\\, x \\, (1-\\alpha) \\quad \\text{ with } |\\alpha|<\\varepsilon.\n",
    "$$\n",
    "\n",
    "In case of decimal representation, one has $\\varepsilon=5 \\times 10^{-m}$.\n",
    "\n",
    "In case of binary representation, one has $\\varepsilon=2^{-m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Floating-point arithmetics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of arithmetics operations (addition, substraction...) needs not be machine numbers, even if the operands are. Then, arthmetic operations can not be achieved exactly on computers and finite-digit operations have to be defined. Let us denote by $\\oplus$, $\\ominus$, $\\otimes$ and $\\oslash$ the finite digit operators corresponding to the addition, substraction, multiplication and division respectivly. The floating-point (or finite-digit) arithmetic is given by\n",
    "\n",
    "\\begin{align}\n",
    "x \\oplus y = rd(\\, rd(x) + rd(y)\\,), &\\quad\\quad\\quad x \\ominus y = rd(\\, rd(x) - rd(y)\\,) \\\\\n",
    "x \\otimes y = rd(\\, rd(x) \\times rd(y)\\,), &\\quad\\quad\\quad x \\oslash y = rd(\\, rd(x)\\,\\, / \\,\\,rd(y)\\,) \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\"> \n",
    "For $\\beta=10$, $n=2$, $m=5$, let us consider $x=1/3$ and $y=6/7$. We have\n",
    "\n",
    "\\begin{align}\n",
    "rd\\left(\\frac{1}{3}\\right)&=rd(0.3333333\\ldots \\times 10^0)=0.33333\\times 10^0\\\\\n",
    "rd\\left(\\frac{6}{7}\\right)&=rd(0.85714285714285\\ldots \\times 10^0)=0.85714\\times 10^0\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{3} \\oplus \\frac{6}{7} \n",
    "&= rd( 0.33333\\times 10^0 + 0.85714\\times 10^0) \\\\\n",
    "&= rd(1.19047\\times 10^0) \\\\\n",
    "&= rd( 0.119047 \\times 10^1) \\\\\n",
    "&= 0.11905 \\times 10^1\n",
    "\\end{align}\n",
    "\n",
    "while the exact value is \n",
    "\n",
    "$$\n",
    "\\frac{1}{3} + \\frac{6}{7} = 0.119047619047619\\ldots\\times 10^1.\n",
    "$$\n",
    "\n",
    "The absolute error is about $2.38\\times 10^{-5}$. The relative error is about $2\\times 10^{-5}$ which is lower than the expected machine precision for $m=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, the results obtained using finite-digit arithmetics are sufficiently accurate for the use we have in mind. However, any computer user has to be aware of some (possibly) problematic consequences of these finite digit computations. The two most common of them are illustrated in the following examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\"> **Addition of a large and a small number: large absolute error.**\n",
    "\n",
    "For $\\beta=10$, $n=2$, $m=5$, let us consider $x=1/3$ and $y=6/7 \\times 10^4$. We have\n",
    "\n",
    "\\begin{align}\n",
    "rd\\left(\\frac{1}{3}\\right)&=rd(0.3333333\\ldots \\times 10^0)=0.33333\\times 10^0\\\\\n",
    "rd\\left(\\frac{6}{7}\\times 10^4\\right)&=rd(0.85714285714285\\ldots \\times 10^4)=0.85714\\times 10^4\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{3} \\oplus \\left(\\frac{6}{7}\\times 10^4\\right) \n",
    "&= rd( 0.33333\\times 10^0 + 0.85714\\times 10^4) \\\\\n",
    "&= rd(8571.73333) \\\\\n",
    "&= rd( 0.857173333 \\times 10^4)\\\\\n",
    "&= 0.85717\\times 10^5 = 8571.7\n",
    "\\end{align}\n",
    "\n",
    "while the exact value is \n",
    "\n",
    "$$\n",
    "\\frac{1}{3} + \\frac{6}{7}\\times 10^4 = 8571.761904761905\\ldots.\n",
    "$$\n",
    "\n",
    "The relative error is about $7.22\\times 10^{-6}$ but **the absolute error is about $0.062$**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\"> **Substraction of nearly equal numbers: large relative error.**\n",
    "\n",
    "For $\\beta=10$, $n=2$, $m=5$, let us consider $x=0.34523$ and $y=0.3451162368$. We have\n",
    "\n",
    "\\begin{align}\n",
    "rd\\left(x\\right)&=0.34523\\\\\n",
    "rd\\left(y\\right)&=0.34512\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "x \\ominus y \n",
    "&= rd( 0.34523 - 0.34512) \\\\\n",
    "&= rd(0.00011) \\\\\n",
    "&= 0.11\\times 10^{-3}\n",
    "\\end{align}\n",
    "\n",
    "while the exact value is \n",
    "\n",
    "$$\n",
    "x-y = 0.1137632\\times 10^{-3}.\n",
    "$$\n",
    "\n",
    "The absolute error is $0.37632\\times 10^{-5}$. **The relative error is about $0.033$ which is much greater than the expected precision for $m=5$. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python example illustrates this behavior with the computer precision. Suppose that you want to compute $$s=x-y$$ for\n",
    "\n",
    "\\begin{align}\n",
    "x&=1+10^{-15}\\\\\n",
    "y&=1\n",
    "\\end{align}\n",
    "The exact result is of course $s=10^{-15}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the following script to compute $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute error = Ellipsis \n",
      "relative error = Ellipsis\n"
     ]
    }
   ],
   "source": [
    "## substraction of two nearly equal numbers\n",
    "\n",
    "# definition of x\n",
    "x = ...\n",
    "# definition of y\n",
    "y = ...\n",
    "# definition the exact result\n",
    "exact_result = ...\n",
    "# compute s\n",
    "numerical_result = ...\n",
    "# compute absolute error\n",
    "absolute_error = ...\n",
    "# compute relative error\n",
    "relative_error = ...\n",
    "# print the errors\n",
    "print('absolute error =', absolute_error, '\\nrelative error =', relative_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for high (but finite) computer precision, the substraction of two nearly equal numbers\n",
    "leads to a relative error of 11% !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional problem that arises when one substracts nearly equal numbers is the **cancellation of significant digits**.  In the previous example, even though $m=5$, $x \\ominus y $ only have two significant digits. Indeed, three digits have been cancelled due to the fact that the first three significant digits of $x$ and $y$ are equal. In most of the cases, since $m=5$, the three missing digits will be assigned by the machine (either zero or randomly assigned). As a consequence, all the following computations using $x-y$ will be carried out using these three non-significant digits, leading to results with at most two significant digits, even if $m=5$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being aware of such behaviours, the user can avoid the loss of accuracy due to round-off errors by taking care of the order for the different operations or by reformulating the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifying the order of operations to increase accuracy**\n",
    "\n",
    "Suppose that you want to compute $$s=a+b+c$$ for\n",
    "\n",
    "\\begin{align}\n",
    "a&=10^{-15}\\\\\n",
    "b&=1\\\\\n",
    "c&=-1\n",
    "\\end{align}\n",
    "The exact result is of course $s=10^{-15}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = (a+b)+c = 1.1102230246251565e-15 [exact value = a+b+c = 1e-15]\n"
     ]
    }
   ],
   "source": [
    "## sum of three terms, first method\n",
    "\n",
    "a = 1e-15\n",
    "b = 1\n",
    "c = -1\n",
    "s1 = (a+b) + c\n",
    "print('s = (a+b)+c =', s1, '[exact value = a+b+c = 1e-15]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding first $a$ and $b$ and then adding $c$ to the previous result leads to a substraction of two nearly equal numbers and a relative error of order $11\\%$. Almost all the digits of the result are not significant... To overcome this problem, it suffices to sum $b$ and $c$ and then add $a$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = a+(b+c) = 1e-15 [exact value = a+b+c = 1e-15]\n"
     ]
    }
   ],
   "source": [
    "## sum of three terms, second method\n",
    "\n",
    "a = 1e-15\n",
    "b = 1\n",
    "c = -1\n",
    "s2 = a + (b+c)\n",
    "print('s = a+(b+c) =', s2, '[exact value = a+b+c = 1e-15]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark that we showed here that the finite-digit addition in not associative !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reformulation of the problem to increase accuracy**\n",
    "\n",
    "Suppose you want to compute the roots $(x_1,x_2)$ of the quadratic polynomial\n",
    "\n",
    "$$\n",
    "ax^2+bx+c=0,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "x_1&=123*10^{-10}\\\\\n",
    "x_2&=10^5\\\\\n",
    "a&=1\\\\\n",
    "b&=-(x_1+x_2)=-10^5-123*10^{-10}\\\\\n",
    "c&=x_1x_2=123*10^{-5}\n",
    "\\end{align}\n",
    "\n",
    "The quadratic formula gives\n",
    "\n",
    "\\begin{align}\n",
    "x_1&=\\frac{-b-\\sqrt{b^2-4ac}}{2a}\\\\\n",
    "x_2&=\\frac{-b+\\sqrt{b^2-4ac}}{2a}\n",
    "\\end{align}\n",
    "\n",
    "In the previous example, we have $b^2>>4ac$ so that the formula for $x_1$ leads to the substraction of two nearly equal numbers ($b$ is negative). The following program computes $x_1$ and $x_2$ from these formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the following script to compute $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 =  Ellipsis [exact value = 1.23e-8] \n",
      "x2 =  Ellipsis [exact value = 100000]\n"
     ]
    }
   ],
   "source": [
    "## roots of quadratic polynoms, classical formulation\n",
    "\n",
    "x1 = 123*1e-10\n",
    "x2 = 1e5\n",
    "a = 1\n",
    "b = -100000.0000000123\n",
    "c = 123*1e-5\n",
    "# compute x1\n",
    "x1num = ...\n",
    "# compute x2\n",
    "x2num = ...\n",
    "print('x1 = ', x1num, '[exact value = 1.23e-8] \\nx2 = ', x2num, '[exact value = 100000]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so, we obtain a relative error of order $8\\%$ for $x_1$... \n",
    "\n",
    "A method to avoid this loss of accuracy is to compute the \"good\" root (in our case $x_2$, it depends on the sign of $b$) using the previous formula and then, use the equality $c=x_1x_2$ to compute the second one: \n",
    "\n",
    "\\begin{align}\n",
    "\\tilde x_2&=\\frac{-b-sign(b)\\sqrt{b^2-4ac}}{2a}\\\\\n",
    "\\tilde x_1&=\\frac{c}{x_1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Use these new formula to compute $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 =  Ellipsis [exact value = 100000] \n",
      "x1 =  Ellipsis [exact value = 1.23e-8]\n"
     ]
    }
   ],
   "source": [
    "## roots of quadratic polynoms, accurate formulation\n",
    "\n",
    "x1 = 1e5\n",
    "x2 = 123*1e-10\n",
    "a = 1\n",
    "b = -100000.0000000123\n",
    "c = 123*1e-5\n",
    "# compute x2\n",
    "x2num2 = ...\n",
    "# compute x1\n",
    "x1num2 = ...\n",
    "print('x2 = ', x2num2, '[exact value = 100000] \\nx1 = ', x1num2, '[exact value = 1.23e-8]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new computation is now accurate up to machine precision for both of the two roots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"trunc\"></a>\n",
    "## Mathematical approximations: truncation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section presented one of the source of errors (round-off errors) appearing when one tries to solve numerically a problem. All the algorithms proposed were supposed to give the exact solution to the problem if performed using exact arithmetics. For a wide class of problems, there does not exist exact algorithms to compute the solution. In that case, one has to design algorithms capable to approximate the solution. One of the main challenge in that case is to obtain algorithms giving \"good\" approximations, that is with \"small\" truncation errors. Before going further, let us define what is an algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Def\">\n",
    "**Algorithm.**\n",
    "\n",
    "An algorithm is a set of directives specifying how to solve (or approximate the solution to) a given problem. It must have the feature of terminating after a finite number of elementary operations.\n",
    "\n",
    "The inputs of the algorithm are given at its beginning. Some of the inputs are given by the problem you want to solve. There can also be other inputs, called **discretization parameters**, which are given by the numerical method chosen to approximate the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the procedures described in the above section to compute $s=a+b+c$ or the roots of quadratic polynomials are algorithms. The two procedures to compute $s$ can be described as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Algo\">\n",
    "**Algorithms to compute $s=a+b+c$.**\n",
    "\n",
    "Algorithm 1:\n",
    "\\begin{align}\n",
    "INPUT:&\\quad a,b,c\\\\\n",
    "DO:&\\quad s=a+b\\\\\n",
    "&\\quad s=s+c\\\\\n",
    "RETURN:&\\quad s\\\\\n",
    "\\end{align}\n",
    "\n",
    "Algorithm 2:\n",
    "\\begin{align}\n",
    "INPUT:&\\quad a,b,c\\\\\n",
    "DO:&\\quad s=b+c\\\\\n",
    "&\\quad s=s+a\\\\\n",
    "RETURN:&\\quad s\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two algorithms are exacts: if performed using exact arithmetics, they are supposed to return the same exact value $s$. The inputs $a$, $b$ and $c$ are given by the problem and no discretization parameters are needed.\n",
    "\n",
    "Suppose now that you want to compute $x^*$ solution to a given problem that cannot be exactly computed using elementary operations. In that case, one can use an approximation of $x^*$, depending on a discretization parameter. For example, suppose you want to compute $x^*=f'(a)$, $f$ being a derivable funcion and $a$ a given real.\n",
    "\n",
    "Several approximations of $f'(a)$ can be suggested by recalling that the derivative of $f$ in $a$ is the slope of the tangent to the graph of $f$ at point $a$. This slope can be approximated by the slope of secants to the graph of $f$ passing through two points of the graph, close to point $(a,f(a))$.\n",
    "\n",
    "<img src=\"./figures/ApproxDer.png\" alt=\"ApproxDer\" style=\"width: 450px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the two following approximations. $x_h$ and $\\bar x_h$ correspond to the approximation using the slope of secant 1 and 2 respectively.\n",
    "Also complete the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, $x^*=f'(a)$ can be approximated for small $h$ by the formula\n",
    "\n",
    "$$\n",
    "f'(a) \\approx x_h = \\frac{f(a+h)-f(a)}{h}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "    f'(a) \\approx \\bar x_h = \\frac{f(a+h)-f(a-h)}{h}\n",
    "$$\n",
    "\n",
    "This leads to the two following algorithms, returning $x_h$ and $\\bar x_h$ respectively for a given value of $h$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Algo\">\n",
    "**Algorithm 1 to compute $x_h$, approximation of $x^*=f'(a)$.**\n",
    "\n",
    "\\begin{align}\n",
    "INPUT:  & \\quad a, f, h  \\\\\n",
    "DO:     & \\quad x = ...  \\\\\n",
    "RETURN: & \\quad x        \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Algo\">\n",
    "**Algorithm 2 to compute $\\bar x_h$, approximation of $x^*=f'(a)$.**\n",
    "\n",
    "\\begin{align}\n",
    "INPUT:  & \\quad a, f, h \\\\\n",
    "DO:     & \\quad x = ... \\\\\n",
    "RETURN: & \\quad x       \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs of the algorithms are of two types: the problem parameters $a$ and $f$, and the discretization parameter $h$. In that case, even using exact arithmetics, the result is an approximation of $x^*$. Its quality depends on the discretization parameter $h$ and one would like to estimate the corresponding errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Def\">\n",
    "** Truncation error**\n",
    "\n",
    "Consider an algorithm designed to compute an approximation of the solution $x^*$ to a given problem. Denote by $\\eta$ the list of discretization parameters given as inputs to the algorithm (i.e. inputs of the algorithm not given by the initial problem). If $x_\\eta$ is the approximation of $x^*$ computed using these discretization parameters, then the *truncation error* for  these parameters is\n",
    "\n",
    "$$\n",
    "e_\\eta=\\left|\\, x^*-x_\\eta \\,\\right|.\n",
    "$$\n",
    "\n",
    "This error is evaluated supposing that $x_\\eta$ is computed using exact arithmetics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Ex\">\n",
    "The parameter driving the precision in the algorithms computing $x^*=f'(a)$ is $h$. The truncation error for these algorithms depends on $h$:\n",
    "\n",
    "$$e_h=\\left|\\, f'(a)-x_h \\,\\right| \\quad \\text{ and } \\quad \\bar e_h=\\left|\\, f'(a)-\\bar x_h \\,\\right|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of an algorithm depends on the behavior of the truncation error. Of course, the objective is to compute approximations of $x^*$ as precise as possible and converging to $x^*$ in some sense (i.e. the error converging to zero). In that case we say that **the algorithm converges**.\n",
    "\n",
    "Let us use the two previous algorithms proposed to compute $x^*=f'(a)$ for $f(x)=x^5$ and $a=1$. We define above the corresponding functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the following function $f: x\\to f(x)$. If the input is a vector $x=(x_i)_i$, it has to return the vector $(f(x_i))_i$. Take care not to use any loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2) = 32\n",
      "f([2 3]) = [ 32 243]\n"
     ]
    }
   ],
   "source": [
    "## Function f: x -> x^5\n",
    "## input : x = discretization parameter (can be a real or an array of reals)\n",
    "## output : real f(x)  if x is real\n",
    "##          array (f(xi))_i if x=(xi)_i is an array\n",
    "import numpy as np\n",
    "def f(x):\n",
    "    return x**5\n",
    "\n",
    "# Test \n",
    "x=2\n",
    "print('f(2) =',f(x))\n",
    "x=np.array([2,3])\n",
    "print('f([2 3]) =',f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the two following functions that return the two approximations of the derivative of $f$ at point $a$, with discretization parameter $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that computes x_h = (f(a+h)-f(a))/h for a given a\n",
    "## input : f = function \n",
    "##         a = point where the derivative has to be approximated\n",
    "##         h = discretization parameter (can be a real or an array of reals)\n",
    "## output : real x_h = (f(a+h)-f(a))/h  if h is real\n",
    "##          array x_h such that x_h[i] = (f(a+h[i])-f(a))/h[i] if h is an array\n",
    "\n",
    "def ApproxDerivative1(f, a, h):\n",
    "    return (f(a+h)-f(a))/h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that computes x_h = (f(a+h)-f(a-h))/2h for a given a\n",
    "## input : a = point where the derivative has to be approximated\n",
    "##         h = discretization parameter\n",
    "## output : real x_h = (f(a+h)-f(a-h))/h  if h is real\n",
    "##          array x_h such that x_h[i] = (f(a+h[i])-f(a-h[i]))/h[i] if h is an array\n",
    "\n",
    "def ApproxDerivative2(f, a, h):\n",
    "    return (f(a+h)-f(a-h))/(2*h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the previous functions by computing approximations of $f'(1)=5$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the following script to test the two previous approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= 1 , h= 0.01 , exact value: f'(1)=5\n",
      "Result given by the first algorithm: 5.101005010000015\n",
      "Result given by the second algorithm: 0.0005001000010000011\n"
     ]
    }
   ],
   "source": [
    "## Test of the two formulas computing a derivative\n",
    "\n",
    "a = 1\n",
    "h = 0.01\n",
    "x1 = (f(a+h)-f(a))/h # first algorithm\n",
    "x2 = (f(a+h)-f(a-h))/(2*h) # second algorithm\n",
    "print('a=', a, ', h=', h, ', exact value: f\\'(1)=5')\n",
    "print('Result given by the first algorithm:', x1)\n",
    "print('Result given by the second algorithm:', x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this test, the approximation given by algorithm 2 is better than the one obtained using algorithm 1 for the same value of $h$. To observe more precisely the behavior of the two algorithms, the following code plots the value of the truncation error $e_h$ versus the discretization parameter $h$ when $h$ goes to zero, using a log-log scale:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the following script to compare the errors of the two algorithms versus $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAIACAYAAADZivtAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+JBCmoiIiIgP/T/OAxqjjdGocwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f774c3917b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## behavior of the error for the two formulas computing a derivative\n",
    "\n",
    "# point a\n",
    "a = 1\n",
    "\n",
    "#sequence of h = [1, 1e-1, 1e-2 ... 1e-5]\n",
    "n = np.arange(6)\n",
    "h = 10.**(-n)\n",
    "\n",
    "# approximation of f'(a) using the first algorithm\n",
    "# and the corresponding error (vector)\n",
    "Der1 = ApproxDerivative1(f, a, h)\n",
    "Err1 = abs(Der1 - 5.)\n",
    "\n",
    "# approximation of f'(a) using the second algorithm\n",
    "# and the corresponding error (vector)\n",
    "Der2 = ApproxDerivative2(f, a, h)\n",
    "Err2 = abs(Der2 - 5.)\n",
    "\n",
    "# plot of the errors versus h\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.plot(h,Der1,label='Der1') # plot error 1 versus h\n",
    "plt.plot(h,Der2,label='Der2')  # plot error 2 versus h\n",
    "plt.legend(loc='upper left', fontsize=18)\n",
    "plt.xlabel('h', fontsize=18)\n",
    "plt.ylabel('Absolute error', fontsize=18)\n",
    "plt.title('Convergence of $x_h$ to $x^*=f\\'(a)$ for $f(x)=x^5$ and $a=1$', fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure confirms that both errors $e_h$ and $\\bar e_h$ seem to converge to zero when $h$ goes to zero.\n",
    "\n",
    "This convergence is quicker for the second algorithm than for the first one. Indeed, for $\\bar x_h$, precision $10^{-3}$ is obtained for $h\\approx 10^{-2}$, while it requires a much smaller $h\\approx 10^{-4}$ for $x_h$.\n",
    "\n",
    "_**These numerical observations (convergence, speed of convergence...) have to be confirmed/proved by a theoretical study of the error. However, since the solution to the problem is not known, this error cannot be explicitly  computed and one has prove that it is bounded by a quantity converging to zero. Such a bound, if explicit, is called an error estimator.**_\n",
    "\n",
    "Here, a study of the remainder of Taylor expansions of $f$ about point $a$ can be achieved in order to quantify the error and its dependance on $h$ for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, supposing $f\\in \\cal{C}^2([a,b])$, from Taylor's theorem (see appendix), we have\n",
    "\n",
    "$$\n",
    "\\forall h\\geq 0, \\quad\\exists \\xi_+ \\in ]a,a+h[ \\quad\\text{ such that }\\quad f(a+h) = f(a) + f'(a)\\, h + \\frac{f''(\\xi_+)}{2!}\\,h^2.\n",
    "$$\n",
    "\n",
    "This gives, for $h\\leq 1$\n",
    "\n",
    "$$\n",
    "e_h = \\left|\\, f'(a) - \\frac{f(a+h)-f(a)}{h} \\,\\right| = \\left| \\frac{f''(\\xi_+)}{2!}\\,h \\right| \\leq \\frac{\\sup_{[0,1]}{\\left|f''\\right|}}{2!}\\,h = M_1\\, h\n",
    "$$\n",
    "\n",
    "where $M_1$ is a constant independant of $h$. From this, we prove the convergence of the first algorithm:\n",
    "\n",
    "$$e_h = \\left|\\, f'(a)-x_h \\,\\right|\\to 0 \\quad \\text{ when } \\quad h\\to 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the convergence of the second algorithm, we suppose $f\\in \\cal{C}^3([a,b])$ and use Taylor's theorem at rank $3$:\n",
    "\n",
    "$$\n",
    "\\forall h\\geq 0, \\quad\\exists \\xi_+ \\in ]a,a+h[ \\quad\\text{ such that }\\quad f(a+h) = f(a) + f'(a)\\, h + \\frac{f''(a)}{2!} \\, h^2 +  \\frac{f'''(\\xi_+)}{3!}\\,h^3\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\forall h\\geq 0, \\quad\\exists \\xi_- \\in ]a-h,a[ \\quad\\text{ such that }\\quad f(a-h) = f(a) - f'(a)\\, h + \\frac{f''(a)}{2!} \\, h^2 +  \\frac{f'''(\\xi_-)}{3!}\\,h^3\n",
    "$$\n",
    "\n",
    "This gives, for $h\\leq 1$\n",
    "\n",
    "$$\n",
    "\\bar e_h = \\left|\\, f'(a) - \\frac{f(a+h)-f(a-h)}{2h} \\,\\right| = \\left| \\frac{f'''(\\xi_+)-f'''(\\xi_-)}{2\\times 3!}\\,h^2 \\right|\\leq \\frac{2 \\sup_{[0,1]}{\\left|f''' \\right|}}{2\\times 3!}\\,h^2 = M_2\\, h^2\n",
    "$$\n",
    "\n",
    "where $M_2$ is a constant independant of $h$. From this, we now have the convergence of the second algorithm:\n",
    "\n",
    "$$\\bar e_h = \\left|\\, f'(a)- \\bar x_h \\,\\right|\\to 0 \\quad \\text{ when } \\quad h\\to 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the explicit estimations\n",
    "\n",
    "$$\n",
    "e_h \\leq M_1 h \\quad \\text{and} \\quad \\bar e_h \\leq M_2 h^2\n",
    "$$\n",
    "\n",
    "we can even extract more precise information about the behavior of the error when $h$ goes to zero. \n",
    "\n",
    "For example, we have proved that the bound  behaves like $h$ for the first algorithm (we say that the error is of **order 1**) and  like $h^2$ for the second (we say that the error is of **order 2**).  The **speed of convergence** is quantified by the order of the error in $h$.  We recover here that algorithm 2 converges quicker to $f'(a)$ than algorithm 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explicit bound can also be used to obtain a quantitative information about the precision of the approximation obtained for a given value of $h$. Indeed, in general, the value of $x^*$ is not known and the numerical error cannot be computed. We proved that the error is less than this estimation so the previous explicit bounds can be used to quantify how precise is the numerical result for a given $h$. \n",
    "\n",
    "We have $M_1=40$ and $M_2= 160/6$ and we compare in the following tables the errors and the estimated bounds for the two algorithms:\n",
    "\n",
    "|&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp; $e_h$ &nbsp;&nbsp;|  numerical error | estimated bound|&nbsp;&nbsp; $\\bar e_h$ &nbsp;&nbsp; |  numerical error | estimated bound |\n",
    "|:--------------------:|:--------------------:|:-----------------------:|:------------------:|:------------------:|:--------------------:|:-----------------------:|\n",
    "|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$h=1e+00$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$2.60000000e+01 $ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| $4.0e+01$   | &nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ 1.10000000e+01 $&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | $  2.666...e+01$   |\n",
    "|&nbsp;&nbsp; $h=1e-01$&nbsp;&nbsp; |&nbsp;    | $1.10510000e+00$| $4.0e+00$ |&nbsp;&nbsp; | $ 1.00100000e-01 $ | $ 2.666...e-01 $   |\n",
    "|&nbsp;&nbsp; $h=1e-02$  &nbsp;&nbsp;  |&nbsp;| $1.01005010e-01$| $4.0e-01$    | &nbsp;&nbsp; | $  1.00001000e-03 $ | $ 2.666...e-03 $   |\n",
    "| &nbsp;&nbsp;$h=1e-03$ &nbsp;&nbsp;|&nbsp;   | $1.00100050e-02$| $4.0e-02$    | &nbsp;&nbsp; | $  1.00000007e-05$ | $ 2.666...e-05 $   |\n",
    "|&nbsp;&nbsp; $h=1e-04$ &nbsp;&nbsp;|&nbsp;   | $1.00010000e-03$| $4.0e-03$    | &nbsp;&nbsp; | $ 9.99994576e-08 $ | $ 2.666...e-07 $   |\n",
    "|&nbsp;&nbsp; $h=1e-05$ &nbsp;&nbsp;|&nbsp;   | $1.00001040e-04$| $4.0e-04$    | &nbsp;&nbsp; | $  1.00975228e-09$ | $ 2.666...e-09 $   |\n",
    "\n",
    "One can observe that the numerical error is below the theoretical bound and that this bound is a rather good estimation for the error. **As a consequence, $h$ being given, one can estimate the precision of the approximation given by the two algorithms by using these explicit bounds (error estimators).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The explicit estimation of the error also allows to choose adequately the value of the discretization parameter needed to reach a given precision.** For example, suppose you want to estimate $x^*=f'(a)$ and to be precise up to $10^{-6}$, The previous estimation says that you can reach this precision with $h=10^{-6}/M_1=2.5\\times 10^{-8}$ using the first algorithm and $h=\\sqrt{10^{-6}/M_2}=1.936\\times 10^{-4}$ using the second one.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div markdown=1 class=\"Rmk\">\n",
    "Note that the order $n$ of the method can be recovered numerically from the log-log plot of the error versus $h$: since \n",
    "$$\n",
    "e_h \\leq M h^n,\n",
    "$$ \n",
    "\n",
    "we have\n",
    "$$\n",
    "\\log e_h \\leq n \\log h + \\log M\n",
    "$$\n",
    "\n",
    "and, if the bound is accurate, $n$ is equal to the slope of the plotted line in log-log scale. \n",
    "\n",
    "The order of the method can also be observed in the previous table. Indeed, in case of a method of order $n$, dividing $h$ by $\\alpha$ divides the error by $\\alpha^n$. In our example, when $h$ is divided by $10$, the error is divided by $10$ for the first algorithm and by $100$ for the second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"total\"></a>\n",
    "## Total numerical error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing an approximation of the solution to a given problem, the total error is made of both the error due to the mathematical approximation of the problem (truncation error) and errors due to the use of finite-digit approximations in computers (round-off errors)\n",
    "\n",
    "To illustrate this, let us consider the first algorithm we proposed in the previous section to estimate the derivative $f'(a)$:\n",
    "\n",
    "$$\n",
    "x_h= \\frac{f(a+h)-f(a)}{h}.\n",
    "$$\n",
    "\n",
    "When $h$ is very small, it consists in a substraction of the two nearly equal reals $f(a+h)$ and $f(a)$ which can induce a high round-off error and this error is increased due to the division by the small real $h$.\n",
    "\n",
    "The approximation of $f'(a)$ given by the computer is in fact\n",
    "\n",
    "$$\n",
    "\\tilde x_h = rd\\left(\\,\\,(\\,\\,rd(f(a+h))\\ominus rd(f(a))\\,\\,)\\,\\,\\oslash\\,\\, rd(h)\\,\\,\\right)\n",
    "$$\n",
    "\n",
    "so that the error can be spilt in two terms:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde e_h &= \\left|\\, f'(a) - \\tilde x_h \\,\\right| \\\\\n",
    "&= \\left|\\, f'(a) - \\frac{f(a+h)-f(a)}{h} + \\frac{f(a+h)-f(a)}{h} - \\tilde x_h\\,\\right| \\\\\n",
    "&\\leq \\left|\\, f'(a) - \\frac{f(a+h)-f(a)}{h}\\,\\right| + \\left|\\,\\frac{f(a+h)-f(a)}{h} - \\tilde x_h \\,\\right|\\\\\n",
    "& \\quad\\quad \\text{Truncation error} \\quad\\quad+\\quad\\quad \\text{Round-off error}\n",
    "\\end{align}\n",
    "\n",
    "We already proved that the truncation error goes to zero when $h$ goes to zero. On the contrary, the round-off error increases for $h$ very small. As a consequence, for $h$ too small, the round-off error can become greater than the truncation error, which deteriorates the approximation. This can be observed for both of the previous algorithms by plotting the error versus $h$ for smaller values of $h$ than in the previous study:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Run the following cell to observe the total numerical error for a wide range of $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ea205793de3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# plot of the errors versus h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Algorithm 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Algorithm 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "## behavior of the error (small h) for the two formulas computing a derivative\n",
    "\n",
    "# point a\n",
    "a = 1\n",
    "\n",
    "#sequence of h = [1, 1e-1, 1e-2 ... 1e-5]\n",
    "n = np.arange(15)\n",
    "h = 10.**(-n)\n",
    "\n",
    "\n",
    "# approximation of f'(a) using the first algorithm\n",
    "# and the corresponding error\n",
    "Der1 = ApproxDerivative1(f, a, h)\n",
    "Err1 = abs(Der1 - 5.)\n",
    "\n",
    "# approximation of f'(a) using the second algorithm\n",
    "# and the corresponding error\n",
    "Der2 = ApproxDerivative2(f, a, h)\n",
    "Err2 = abs(Der2 - 5.)\n",
    "\n",
    "# plot of the errors versus h\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.loglog(h, Err1, marker=\"o\", label=\"Algorithm 1\")\n",
    "plt.loglog(h, Err2, marker=\"o\", label=\"Algorithm 2\")\n",
    "plt.legend(loc='upper left', fontsize=18)\n",
    "plt.xlabel('h', fontsize=18)\n",
    "plt.ylabel('Absolute error', fontsize=18)\n",
    "plt.title('Convergence of $x_h$ to $x^*=f\\'(a)$ for $f(x)=x^5$ and $a=1$', fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sufficiently big values of $h$, the error is dominated by the truncation error: we observe the convergence to zero of the error with order $1$ or $2$ depending on the algorithm. Then, below a given value of $h$, the round-off error becomes dominant and the total error increase... One has to be aware of such behaviors to be able to chose efficiently the discretization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor's theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "        <tr>\n",
    "            <td><img src=\"figures/Brook_Taylor.jpg\" alt=\"Taylor\" style=\"width: 170px;\" /></td>\n",
    "            <td><img src=\"figures/Lagrange.jpg\" alt=\"Lagrange\" style=\"width: 150px;\" /></td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    ">**Brook Taylor (1685  1731) and Joseph-Louis Lagrange (1736 - 1813).**\n",
    ">Brook Taylor is an english mathematician and was also musician and painter artist. He is best known for the so-called Taylor's theorem and Taylor series that can be found in its book *Methodus incrementorum directa et inversa* (without rest nor convergence result) . In this book, one can also found the integration by parts formula and the prinicples of finite differences. It remained quite unknown until the italian mathematician and astronomer Joseph-Louis Lagrange discoverd its importance in 1772, to the point of considering Taylor's work to be the *foundation of differential calculus*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall below Taylor's theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  markdown=1 class=\"Thm\">\n",
    "** Taylor's Theorem**\n",
    "\n",
    "Suppose $f\\in C^n[a,b]$, $f^{(n+1)}$ exists on $[a,b]$ and $x_0\\in [a,b]$. For every $x\\in [a,b]$, there exists $\\xi(x)$ between $x_0$ and $x$ with \n",
    "\n",
    "$$\n",
    "f(x) = P_n(x) + R_n(x),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align}\n",
    "P_n(x) &= f(x_0) + f'(x_0) (x-x_0) + \\frac{f''(x_0)}{2!} (x-x_0)^2 + \\ldots +  \\frac{f^{(n)}(x_0)}{n!} (x-x_0)^n\\\\\n",
    "&=\\sum_{k=0}^n \\frac{f^{(k)}(x_0)}{k!} (x-x_0)^k,\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "R_n(x) = \\frac{f^{(n+1)}(\\xi(x))}{(n+1)!} (x-x_0)^{n+1}.\n",
    "$$\n",
    "\n",
    "$P_n$ is called the **$n$-th Taylor polynomial** for $f$ about $x_0$ and $R_n$ is called the **remainder term** associated with $P_n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './style/custom2.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bfbec648f467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstyles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./style/custom2.css\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-bfbec648f467>\u001b[0m in \u001b[0;36mcss_styling\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstyles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./style/custom2.css\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './style/custom2.css'"
     ]
    }
   ],
   "source": [
    "# execute this part to modify the css style\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./style/custom2.css\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
